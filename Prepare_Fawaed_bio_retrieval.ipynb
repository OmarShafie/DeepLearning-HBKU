{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Prepare_Fawaed_bio_retrieval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMhRsW60Uw2vXvCwDRklaeg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmarShafie/HBKU_Projects/blob/master/Prepare_Fawaed_bio_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmuy-uZCxXc3",
        "outputId": "6a575afe-ee6b-429c-f756-b336af99c8b9"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Feb 15 14:33:20 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgYTySbWA-1c",
        "outputId": "9b2f6daa-f215-4880-e1be-2ef7db8f3ac9"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUv7kUDLYXmi",
        "outputId": "60d3a3d6-00e0-4c5e-806d-81223fde075a"
      },
      "source": [
        "!pip install transformers\n",
        "!git clone https://github.com/aub-mind/arabert\n",
        "!pip install pyarabic\n",
        "!pip install farasapy\n",
        "!pip install python-bidi\n",
        "!pip install arabic_reshaper"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 48.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 56.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=f4b44f4043259554ec00334715f805dc390ee53871804026e7a8512dde2e6586\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n",
            "Cloning into 'arabert'...\n",
            "remote: Enumerating objects: 206, done.\u001b[K\n",
            "remote: Counting objects: 100% (206/206), done.\u001b[K\n",
            "remote: Compressing objects: 100% (151/151), done.\u001b[K\n",
            "remote: Total 420 (delta 98), reused 152 (delta 49), pack-reused 214\u001b[K\n",
            "Receiving objects: 100% (420/420), 3.82 MiB | 6.93 MiB/s, done.\n",
            "Resolving deltas: 100% (221/221), done.\n",
            "Collecting pyarabic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/e2/46728ec2f6fe14970de5c782346609f0636262c0941228f363710903aaa1/PyArabic-0.6.10.tar.gz (108kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 8.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyarabic\n",
            "  Building wheel for pyarabic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyarabic: filename=PyArabic-0.6.10-cp36-none-any.whl size=113324 sha256=1e26d992d76146b6813eb28929b2e9a83aae8a24223209f28b1a1ed5bf799805\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/b8/f5/b7c1a50e6efb83544844f165a9b134afe7292585465e29b61d\n",
            "Successfully built pyarabic\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.10\n",
            "Collecting farasapy\n",
            "  Downloading https://files.pythonhosted.org/packages/76/5a/2fdab6860fa66e4ea8258e627af44610bc51465214b3f374079c0e0ad404/farasapy-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from farasapy) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from farasapy) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->farasapy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->farasapy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->farasapy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->farasapy) (2.10)\n",
            "Installing collected packages: farasapy\n",
            "Successfully installed farasapy-0.0.12\n",
            "Collecting python-bidi\n",
            "  Downloading https://files.pythonhosted.org/packages/33/b0/f942d146a2f457233baaafd6bdf624eba8e0f665045b4abd69d1b62d097d/python_bidi-0.4.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from python-bidi) (1.15.0)\n",
            "Installing collected packages: python-bidi\n",
            "Successfully installed python-bidi-0.4.2\n",
            "Collecting arabic_reshaper\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/1e/b78cacd22bd55c43f2711115d67ece3ed9616ba82d2d0e0914db2a6db985/arabic_reshaper-2.1.1.tar.gz\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from arabic_reshaper) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from arabic_reshaper) (53.0.0)\n",
            "Building wheels for collected packages: arabic-reshaper\n",
            "  Building wheel for arabic-reshaper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for arabic-reshaper: filename=arabic_reshaper-2.1.1-cp36-none-any.whl size=16504 sha256=08fbebedb062e18277803e4c5c663399058c82718429fb6b68ec50b209325108\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/52/d8/bd0dcbf00f9e77e3bd0184285ed77dfa9c475dac494a5353d1\n",
            "Successfully built arabic-reshaper\n",
            "Installing collected packages: arabic-reshaper\n",
            "Successfully installed arabic-reshaper-2.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q68NwiniBVAn",
        "outputId": "55d03cdb-9605-4ff2-de11-72c1735a45ca"
      },
      "source": [
        "dataset_directory = 'gdrive/MyDrive/HBKU/Research/datasets/tutorial/'\n",
        "!ls '$dataset_directory'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " bio_sen_tagged_test_farasa.csv    rawi_bio.csv\n",
            " bio_sen_tagged_train_farasa.csv   rawi_bio_grade.csv\n",
            " bio_tagged.csv\t\t\t   rawi_bio_grade_old.csv\n",
            " bio_tagged_test.csv\t\t   README.md\n",
            " bio_tagged_test_farasa.csv\t   scrapy.cfg\n",
            " bio_tagged_train.csv\t\t   sen_tagged_test.csv\n",
            " bio_tagged_train_farasa.csv\t   sen_tagged_test_farasa.csv\n",
            "'data - data.csv'\t\t   sen_tagged_train.csv\n",
            " hadith.csv\t\t\t   sen_tagged_train_farasa.csv\n",
            " narrators.csv\t\t\t   spidyquotes-viewstate.py\n",
            " narrators-utf8.csv\t\t   takhreeg.csv\n",
            " quotes-1.html\t\t\t   takhreeg_data.csv\n",
            " quotes-2.html\t\t\t   takhreeg.numbers\n",
            " quotes.json\t\t\t   tutorial\n",
            " rawi_bio_category.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10p8DQbzHSae",
        "outputId": "d41ae75c-3394-4f20-879a-c9b5809a5da5"
      },
      "source": [
        "!pip install cudf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cudf\n",
            "  Downloading https://files.pythonhosted.org/packages/ee/8f/b8f7eb3c24d1062b419fec0b9e39b98a5954b3d0fc1539b8f830565ff06b/cudf-0.6.1.post1.tar.gz\n",
            "Building wheels for collected packages: cudf\n",
            "  Building wheel for cudf (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for cudf\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for cudf\n",
            "Failed to build cudf\n",
            "Installing collected packages: cudf\n",
            "    Running setup.py install for cudf ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-9acg_t3y/cudf/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-9acg_t3y/cudf/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-7rcuio69/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB5cwhbHBeNO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "a5ee6fdb-0d7b-469e-cd93-22193b3c061b"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#viz\n",
        "import matplotlib.pyplot as plt\n",
        "from bidi.algorithm import get_display\n",
        "import arabic_reshaper\n",
        "import matplotlib.gridspec as gridspec \n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-839887faa25c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcudf\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#viz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cudf'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO4ew62lBgSF"
      },
      "source": [
        "path_bio_category = dataset_directory + '/rawi_bio_category.csv'\n",
        "df_bio_category = pd.read_csv(path_bio_category, header=0)\n",
        "df_bio_category.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HMXO3uFObbl"
      },
      "source": [
        "#filter = df_bio_category[\"faeeda_category\"] == 'الإرسال'\n",
        "#df_bio_category = df_bio_category[filter]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IaiuZW-L83s"
      },
      "source": [
        "df_bio_category = df_bio_category[['faeeda_source', 'faeeda_category', 'faeeda_text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t00OiFWGBmVk"
      },
      "source": [
        "path_bio = dataset_directory + '/rawi_bio.csv'\n",
        "df_bio = pd.read_csv(path_bio, header=0)\n",
        "\n",
        "df_bio.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGCgYk3BMHes"
      },
      "source": [
        "df_bio = df_bio[['targama_id', 'targama_text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32JjeXjuBhKL"
      },
      "source": [
        "#clean data: remove duplicates, beutiful soup\n",
        "df_bio_category.drop_duplicates(subset=None, keep=\"first\", inplace=True)\n",
        "df_bio.drop_duplicates(subset=None, keep=\"first\", inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNFIDWGlAgyi"
      },
      "source": [
        "df_bio_category = df_bio_category.fillna(0)\n",
        "df_bio_category = df_bio_category.astype({'faeeda_source': 'int32'})\n",
        "df_bio = df_bio.astype({'targama_id': 'int32'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUogkC2DXkjM"
      },
      "source": [
        "df_bio_category['faeeda_text'] = df_bio_category['faeeda_text'].apply(lambda x: BeautifulSoup(x, \"lxml\").text)\n",
        "df_bio['targama_text'] = df_bio['targama_text'].apply(lambda x: BeautifulSoup(x, \"lxml\").text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coyfHjt8PjBP"
      },
      "source": [
        "#df_bio_category['faeeda_text'] = df_bio_category['faeeda_text'].apply(lambda x: farasa.segment(x))\n",
        "#df_bio['targama_text'] = df_bio['targama_text'].apply(lambda x: farasa.segment(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVBOwi6gI9dH"
      },
      "source": [
        "cats = df_bio_category['faeeda_category'].value_counts()\n",
        "cat_names = list(cats.index)\n",
        "class_names=list(df_bio_category.groupby('faeeda_category').nunique().index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNylkWrBJQ7w"
      },
      "source": [
        "plt.figure()\n",
        "\n",
        "count = list(cats)\n",
        "\n",
        "reshaped_categories = [get_display(arabic_reshaper.reshape(cat)) for cat in cat_names]\n",
        "ax= sns.barplot(y=reshaped_categories, x=count, alpha=0.8)\n",
        "plt.title('Type ')\n",
        "plt.ylabel(\"# per class\", fontsize=12)\n",
        "plt.xlabel('# of Occurrences', fontsize=12)\n",
        "#adding the text labels\n",
        "rects = ax.patches\n",
        "for rect, label in zip(rects, count):\n",
        "    ax.text(rect.get_x() + rect.get_width() + 10,  rect.get_y() + rect.get_height(), label)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfvcWjQZWnFN"
      },
      "source": [
        "import re\n",
        "def split_sentences(text):\n",
        "  return text.split(\"/n\") #re.split('\\.|،',text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7dpAgPVBqtD"
      },
      "source": [
        "clean_tags = [0 for label in range(len(cat_names))]\n",
        "targama_dict = {}\n",
        "for b in range(len(df_bio)):\n",
        "  sentences = [s.strip() for s in split_sentences(df_bio[\"targama_text\"].iat[b]) if len(s.strip()) > 0 and len(s.strip()) < 1200]\n",
        "  labels = [clean_tags.copy() for s in range(len(sentences))]\n",
        "  targama_dict[df_bio[\"targama_id\"].iat[b]] = (sentences,labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vSKYNDsd8A8"
      },
      "source": [
        "len(df_bio_category[\"faeeda_source\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCZZLADmRr4k"
      },
      "source": [
        "targama_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj8aAO-ewQCA"
      },
      "source": [
        "def update_tags(mylist, pattern, tag, old_tags):\n",
        "  new_tags = old_tags[:]\n",
        "  cat_idx = cat_names.index(tag)\n",
        "  for i in range(len(mylist)):\n",
        "      if len(pattern) > 0 and pattern[0] in mylist[i]:\n",
        "        for j in range(len(pattern)):\n",
        "          new_tags[i+j][cat_idx] = 1\n",
        "        return new_tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO-Ov10WDaWu"
      },
      "source": [
        "#sanity check\n",
        "test_row = 42\n",
        "test_index = df_bio_category[\"faeeda_source\"].iat[test_row]\n",
        "test_pattern = split_sentences(df_bio_category[\"faeeda_text\"].iat[test_row])\n",
        "test_list = targama_dict[test_index][0]\n",
        "for i in test_list:\n",
        "  print(i) \n",
        "update_tags(test_list,test_pattern,df_bio_category[\"faeeda_category\"].iat[test_row], targama_dict[test_index][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJVWbL8R-l1u"
      },
      "source": [
        "missing = 0\n",
        "found = 0\n",
        "duplicates = 0\n",
        "for t in range(len(df_bio_category)):\n",
        "  index = df_bio_category[\"faeeda_source\"].iat[t]\n",
        "  if index in targama_dict:\n",
        "    if not any([any(labels) for labels in targama_dict[index][1]]):\n",
        "      found += 1\n",
        "    else:\n",
        "      duplicates += 1\n",
        "    update_tags(targama_dict[index][0],\n",
        "                [s.strip() for s in split_sentences(df_bio_category[\"faeeda_text\"].iat[t]) if len(s.strip()) > 0 and len(s.strip()) < 1200], \n",
        "                df_bio_category[\"faeeda_category\"].iat[t],\n",
        "                targama_dict[index][1])\n",
        "  else:\n",
        "    missing += 1\n",
        "print(\"missing\", missing)\n",
        "print(\"found\", found)\n",
        "print(\"duplicates\",duplicates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19yz9tqi2Fan"
      },
      "source": [
        "tagged_sen_dict = [(targama_dict[k][0][i],targama_dict[k][1][i]) for k in targama_dict.keys() for i in range(len(targama_dict[k][0]))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2NHBNh_Ngjh"
      },
      "source": [
        "biography_data = pd.DataFrame.from_dict(targama_dict, orient='index')\n",
        "sentences_data = pd.DataFrame(tagged_sen_dict, columns=['text', 'labels'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-Ub5D07Ck-6"
      },
      "source": [
        "sentences_data[[cat for cat in cat_names]] = pd.DataFrame(sentences_data['labels'].to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coQPp_gjAT-G"
      },
      "source": [
        "sentences_data = sentences_data.drop(columns=['labels'])\n",
        "sentences_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVaLHPJf9Sa7"
      },
      "source": [
        "x=sentences_data.iloc[:,1:].sum()\n",
        "#marking comments without any tags as \"clean\"\n",
        "rowsums=sentences_data.iloc[:,1:].sum(axis=1)\n",
        "sentences_data['clean']=(rowsums==0)\n",
        "#count number of clean entries\n",
        "sentences_data['clean'].sum()\n",
        "print(\"Total comments = \",len(sentences_data))\n",
        "print(\"Total clean comments = \",sentences_data['clean'].sum())\n",
        "print(\"Total tags =\",x.sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL-0Ub_j9jX7"
      },
      "source": [
        "x=sentences_data.iloc[:,1:].sum()\n",
        "x=rowsums.value_counts()\n",
        "\n",
        "#plot\n",
        "plt.figure(figsize=(8,4))\n",
        "ax = sns.barplot(x.index, x.values, alpha=0.8)\n",
        "plt.title(\"Multiple tags per comment\")\n",
        "plt.ylabel('# of Occurrences', fontsize=12)\n",
        "plt.xlabel('# of tags ', fontsize=12)\n",
        "\n",
        "#adding the text labels\n",
        "rects = ax.patches\n",
        "labels = x.values\n",
        "for rect, label in zip(rects, labels):\n",
        "    height = rect.get_height()\n",
        "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Pq3eSHy-k85"
      },
      "source": [
        "sentences_data.rename( columns={'':'text'}, inplace=True )\n",
        "sentences_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuyy77CKLck9"
      },
      "source": [
        "!pip install ar_wordcloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ws4nbEM-Rp5"
      },
      "source": [
        "#wordcloud for clean comments\n",
        "from ar_wordcloud import ArabicWordCloud\n",
        "for cat in range(len(cat_names)):\n",
        "  subset=sentences_data[sentences_data[cat_names[cat]]==1][\"text\"].apply(lambda x: get_display(arabic_reshaper.reshape(x)))\n",
        "  text=subset.values\n",
        "  wc= ArabicWordCloud(background_color=\"black\",max_words=2000).generate(\" \".join(text))\n",
        "  plt.figure(figsize=(20,10))\n",
        "  plt.axis(\"off\")\n",
        "  plt.title(\"Words frequented in\"+ reshaped_categories[cat] +\" fawaed\", fontsize=20)\n",
        "  plt.imshow(wc)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7cY7HJm0RLj"
      },
      "source": [
        "sentences_data = sentences_data[sentences_data['clean'] == False]\n",
        "sentences_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dotCWCN4y2nL"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "from arabert.preprocess import never_split_tokens, ArabertPreprocessor\n",
        "from farasa.segmenter import FarasaSegmenter\n",
        "import torch\n",
        "\n",
        "arabert_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"aubmindlab/bert-base-arabertv2\",\n",
        "    do_lower_case=False,\n",
        "    do_basic_tokenize=True,\n",
        "    never_split=never_split_tokens)\n",
        "arabert_model = AutoModel.from_pretrained(\"aubmindlab/bert-base-arabertv2\") #you can replace the path here with the folder containing the the pytorch model\n",
        "\n",
        "farasa_segmenter = FarasaSegmenter(interactive=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg6MwyVMy6lu"
      },
      "source": [
        "model_name = \"bert-base-arabertv2\"\n",
        "arabert_prep = ArabertPreprocessor(model_name=model_name, keep_emojis=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOOsWOrny_my"
      },
      "source": [
        "from scipy import spatial\n",
        "\n",
        "def vectorize(s):\n",
        "  text_preprocessed = arabert_prep.preprocess(s)\n",
        "  arabert_input = arabert_tokenizer.encode(text_preprocessed)#,add_special_tokens=True, max_length=512, pad_to_max_length=True)\n",
        "  tensor_input_ids = torch.tensor(arabert_input).unsqueeze(0)\n",
        "  output = arabert_model(tensor_input_ids)\n",
        "  embeddings = output[0][0][1:-1]\n",
        "  return torch.mean(embeddings, dim=0).detach().numpy()\n",
        "\n",
        "sentences = [\n",
        "    \"قام الطلاب للمعلم\",\n",
        "    \"نهض الطلبة للمدرس\",\n",
        "    \"ذهب الرجل إلى السوق\",\n",
        "]\n",
        "\n",
        "vectors_bert = [vectorize(s) for s in sentences]\n",
        "\n",
        "dist_1 = spatial.distance.cosine(vectors_bert[0], vectors_bert[1])\n",
        "dist_2 = spatial.distance.cosine(vectors_bert[0], vectors_bert[2])\n",
        "print('dist_1: {0}, dist_2: {1}'.format(dist_1, dist_2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueUovekihV_x"
      },
      "source": [
        "!pip install swifter\n",
        "import swifter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLSuCriXzEAm"
      },
      "source": [
        "sentences_data['embeddings'] = sentences_data['text'].swifter.apply(lambda x: vectorize(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FGL9ej125BY"
      },
      "source": [
        "count = 0\n",
        "for i in range(len(sentences_data['text'])):\n",
        "  x = sentences_data['text'].iat[i]\n",
        "  print(i,\"-\",len(x))\n",
        "  count+= 1 if len(x)> 1200 else 0\n",
        "  print(count)\n",
        "  vectorize(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkmGrc6f-oby"
      },
      "source": [
        "sentences_data['text'].iat[1471]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoMiirisz3Sy"
      },
      "source": [
        "sentences_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H966qA6zJHja"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "bio_train, bio_test = train_test_split(biography_data, test_size = 0.2, random_state = 42)\n",
        "sen_train, sen_test = train_test_split(sentences_data, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqBy51mAhIPq"
      },
      "source": [
        "print(len(bio_train),len(bio_test))\n",
        "print(len(sen_train),len(sen_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsH666f-Mzox"
      },
      "source": [
        "bio_train.to_csv('bio_tagged_train.csv')\n",
        "!cp bio_tagged_train.csv 'gdrive/MyDrive/HBKU/Research/datasets/tutorial/'\n",
        "\n",
        "bio_test.to_csv('bio_tagged_test.csv')\n",
        "!cp bio_tagged_test.csv 'gdrive/MyDrive/HBKU/Research/datasets/tutorial/'\n",
        "\n",
        "'''\n",
        "sen_train.to_csv('sen_tagged_train.csv')\n",
        "!cp sen_tagged_train.csv 'gdrive/MyDrive/HBKU/Research/datasets/tutorial/'\n",
        "\n",
        "sen_test.to_csv('sen_tagged_test.csv')\n",
        "!cp sen_tagged_test.csv 'gdrive/MyDrive/HBKU/Research/datasets/tutorial/'\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qR9WAJ2u1LR"
      },
      "source": [
        "To Do:\n",
        "* Expand to multi-label\n",
        "* Visualizaiton and ensure correct types\n",
        "* Clean Data"
      ]
    }
  ]
}